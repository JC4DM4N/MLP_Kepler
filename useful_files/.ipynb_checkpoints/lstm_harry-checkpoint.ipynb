{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "varied-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "medical-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "times   = np.loadtxt('../data/train_test/times.csv', delimiter=',')\n",
    "fluxes  = np.loadtxt('../data/train_test/fluxes.csv', delimiter=',')\n",
    "targets = np.loadtxt('../data/train_test/targets.csv', delimiter=',').astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "grave-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and translate times so that they fall in the range [0,1]\n",
    "times = (times - times[:, 0, np.newaxis]) / (times[:, -1, np.newaxis] - times[:, 0, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "green-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform flux so that it has μ = 0, σ = 1\n",
    "scaler = StandardScaler()\n",
    "fluxes = scaler.fit_transform(fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "provincial-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack([times,fluxes],axis=-1)\n",
    "y = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "multiple-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 128)               67072     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 68,373\n",
      "Trainable params: 68,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(128, activation='relu', input_shape=(1624,2)))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "electoral-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-promotion",
   "metadata": {},
   "source": [
    "This really ought to train, not sure why our model isnt converging. Bit disturbing really. Might need tweak the arguments in layers.LSTM to suit our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "attached-discussion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "38/38 [==============================] - 31s 824ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 31s 818ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 31s 822ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 31s 822ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 31s 822ms/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, epochs=5, validation_split=0.2, batch_size=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "nearby-wrapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [nan, nan, nan, nan, nan], 'val_loss': [nan, nan, nan, nan, nan]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "accessible-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] BATMAAAAAN!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
